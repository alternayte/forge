---
phase: 08-background-jobs-production-readiness
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/sse/limiter.go
  - internal/notify/hub.go
  - internal/notify/subscription.go
autonomous: true
requirements:
  - SSE-01
  - SSE-02
  - SSE-03
  - SSE-04
  - SSE-05
  - SSE-06

must_haves:
  truths:
    - "SSELimiter.Acquire returns release func on success, ErrTooManyConnections when global cap exceeded"
    - "SSELimiter.Acquire returns ErrTooManyConnectionsForUser when per-user cap exceeded"
    - "NotifyHub interface exists with Subscribe, Publish, and Start methods"
    - "PostgresHub implementation uses pgxlisten for single-connection LISTEN/NOTIFY with auto-reconnect"
    - "Fan-out uses non-blocking send; full subscriber buffers drop events and send refresh signal (backpressure)"
    - "Subscription.cancel() unsubscribes and cleans up the subscriber channel"
  artifacts:
    - path: "internal/sse/limiter.go"
      provides: "SSELimiter with global and per-user atomic connection counters"
      contains: "func.*Acquire"
    - path: "internal/notify/hub.go"
      provides: "NotifyHub interface and PostgresHub implementation with pgxlisten"
      contains: "type NotifyHub interface"
    - path: "internal/notify/subscription.go"
      provides: "Subscription type with Events channel and cancel function, Event type"
      contains: "type Subscription struct"
  key_links:
    - from: "internal/notify/hub.go"
      to: "internal/notify/subscription.go"
      via: "PostgresHub.Subscribe creates Subscription with buffered channel and cancel func"
      pattern: "func.*PostgresHub.*Subscribe"
    - from: "internal/sse/limiter.go"
      to: "internal/config/config.go"
      via: "SSELimiter uses SSEConfig.MaxTotalConnections and MaxPerUser"
      pattern: "SSEConfig"
---

<objective>
Create the SSE connection limiter and PostgreSQL LISTEN/NOTIFY hub for real-time event fan-out to SSE clients.

Purpose: Provides the real-time event infrastructure that enables live UI updates via Datastar SSE, with resource exhaustion protection and backpressure handling.
Output: internal/sse/limiter.go with global + per-user connection limits, internal/notify/ package with NotifyHub interface and PostgresHub implementation using pgxlisten.
</objective>

<execution_context>
@/Users/nathananderson-tennant/.claude/get-shit-done/workflows/execute-plan.md
@/Users/nathananderson-tennant/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-background-jobs-production-readiness/08-RESEARCH.md

# Config for SSE limits
@internal/config/config.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SSE connection limiter with global and per-user caps</name>
  <files>
    internal/sse/limiter.go
  </files>
  <action>
Create `internal/sse/limiter.go` with package `sse`:

1. Define error variables:
```go
var (
    ErrTooManyConnections       = errors.New("too many SSE connections")
    ErrTooManyConnectionsForUser = errors.New("too many SSE connections for user")
)
```

2. Define `SSELimiter` struct:
```go
type SSELimiter struct {
    maxTotal   int64
    maxPerUser int64
    active     atomic.Int64
    perUser    sync.Map // string(userID) -> *atomic.Int64
}
```

3. Constructor: `func NewSSELimiter(maxTotal, maxPerUser int) *SSELimiter` — stores int64 values.

4. `func (l *SSELimiter) Acquire(userID string) (release func(), err error)`:
   a. Check `l.active.Load() >= l.maxTotal` — return nil, ErrTooManyConnections if exceeded (SSE-01).
   b. Get or create per-user counter from sync.Map: `l.perUser.LoadOrStore(userID, &atomic.Int64{})` then type assert to `*atomic.Int64`.
   c. Check `userCount.Load() >= l.maxPerUser` — return nil, ErrTooManyConnectionsForUser if exceeded (SSE-02).
   d. Increment both: `l.active.Add(1)`, `userCount.Add(1)`.
   e. Return release func: `func() { l.active.Add(-1); userCount.Add(-1) }`.

5. `func (l *SSELimiter) ActiveConnections() int64` — returns `l.active.Load()` for metrics.

6. `func (l *SSELimiter) ActiveForUser(userID string) int64` — returns per-user count for debugging.

**Note on SSE-03 (graceful shutdown):** Context cancellation is the mechanism — SSE handlers receive ctx from the HTTP request. When server calls Shutdown(), contexts are cancelled, SSE goroutines exit, release() is called. No special shutdown logic needed in the limiter itself.

Run `go get` for any missing stdlib imports (sync, sync/atomic, errors — all stdlib).
  </action>
  <verify>
Run `go build ./internal/sse/...` — compiles.

Run `go vet ./internal/sse/...` — no issues.
  </verify>
  <done>
SSELimiter tracks global and per-user atomic connection counts. Acquire() enforces both caps and returns a release function. ErrTooManyConnections and ErrTooManyConnectionsForUser are exported errors for 429 response handling.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create NotifyHub interface and PostgresHub implementation</name>
  <files>
    internal/notify/hub.go
    internal/notify/subscription.go
  </files>
  <action>
1. Create `internal/notify/subscription.go` with package `notify`:
   - `Event` struct with `Channel string` and `Payload []byte` fields.
   - `Subscription` struct with `Events <-chan Event` (read-only channel for consumer) and `cancel func()`.
   - `func (s *Subscription) Close()` calls s.cancel() to unsubscribe.
   - Internal `internalSub` struct with `ch chan Event` (the writable side).

2. Create `internal/notify/hub.go`:
   - Define `NotifyHub` interface (SSE-06 — swappable):
     ```go
     type NotifyHub interface {
         Subscribe(channel string, tenantID uuid.UUID) *Subscription
         Publish(ctx context.Context, channel string, tenantID uuid.UUID, payload []byte) error
         Start(ctx context.Context) error
     }
     ```
   - Define `notifyMessage` internal struct for JSON encoding/decoding of NOTIFY payloads:
     ```go
     type notifyMessage struct {
         Channel    string          `json:"channel"`
         TenantID   string          `json:"tenant_id"`
         RawPayload json.RawMessage `json:"payload"`
     }
     ```
   - Define `PostgresHub` struct implementing NotifyHub:
     ```go
     type PostgresHub struct {
         connConfig *pgx.ConnConfig
         listener   *pgxlisten.Listener
         bufferSize int
         mu         sync.RWMutex
         subs       map[string][]*internalSub // key = "channel:tenantID"
     }
     ```
   - Constructor: `func NewPostgresHub(connConfig *pgx.ConnConfig, bufferSize int) *PostgresHub` — defaults bufferSize to 32 if 0.

   - `Subscribe(channel string, tenantID uuid.UUID) *Subscription`:
     a. Build key: `channel + ":" + tenantID.String()`
     b. Create buffered chan Event with capacity = bufferSize
     c. Lock mu, append sub to subs[key], unlock
     d. Return Subscription with Events = ch (read-only side) and cancel = closure calling unsubscribe(key, sub)

   - `unsubscribe(key string, sub *internalSub)`:
     a. Lock mu, remove sub from subs[key] slice, unlock
     b. Close(sub.ch)

   - `Publish(ctx context.Context, channel string, tenantID uuid.UUID, payload []byte) error`:
     a. Marshal notifyMessage with channel, tenantID, payload
     b. Execute `SELECT pg_notify($1, $2)` using a pgx connection from ctx or a stored pool reference
     c. **Design note:** Publish needs a DB executor (pool or conn). Add a `db` field to PostgresHub that accepts an interface with `Exec(ctx, sql, args...)` — reuse the existing `actions.DB` interface pattern or define a minimal `Executor` interface.
     d. pg_notify payload max is 8000 bytes — if marshaled message > 8000 bytes, return an error (don't silently truncate).

   - `Start(ctx context.Context) error`:
     a. Create `pgxlisten.Listener` with Connect callback using connConfig
     b. Set ReconnectDelay to 5 * time.Second
     c. Register handler for "forge_events" channel (single channel per research — routing done in payload)
     d. Call `listener.Listen(ctx)` — this blocks until ctx is cancelled (SSE-03: graceful shutdown via context)

   - `handleNotification(ctx context.Context, n *pgconn.Notification, conn *pgx.Conn) error`:
     a. Unmarshal n.Payload into notifyMessage
     b. Build key from msg.Channel + ":" + msg.TenantID
     c. RLock mu, get subs for key, RUnlock
     d. Fan-out to each sub with non-blocking send (SSE-05 backpressure):
     ```go
     for _, sub := range subs {
         select {
         case sub.ch <- Event{Channel: msg.Channel, Payload: msg.RawPayload}:
         default:
             // Buffer full — drop event, send refresh signal
             select {
             case sub.ch <- Event{Channel: "refresh", Payload: nil}:
             default: // Even refresh can't be sent; subscriber is severely behind
             }
         }
     }
     ```

Run `go get github.com/jackc/pgxlisten` to add the dependency.
  </action>
  <verify>
Run `go build ./internal/notify/...` — compiles.

Run `go build ./internal/sse/...` — compiles.

Run `go vet ./internal/notify/...` and `go vet ./internal/sse/...` — no issues.
  </verify>
  <done>
NotifyHub interface defined with Subscribe/Publish/Start. PostgresHub uses pgxlisten for single-connection LISTEN/NOTIFY with auto-reconnect. Fan-out uses non-blocking select for backpressure. SSELimiter enforces global and per-user connection caps. Both packages compile and are ready for integration.
  </done>
</task>

</tasks>

<verification>
1. `go build ./internal/sse/...` — compiles
2. `go build ./internal/notify/...` — compiles
3. `go vet ./internal/sse/... ./internal/notify/...` — no issues
4. NotifyHub interface has Subscribe, Publish, Start methods
5. PostgresHub uses pgxlisten with auto-reconnect
6. SSELimiter has Acquire/release pattern with atomic counters
</verification>

<success_criteria>
- SSELimiter with global cap (default 5000) and per-user cap (default 10) using atomic counters
- Acquire returns release func; exceeding cap returns typed error for 429 handling
- NotifyHub interface supports backend swapping (SSE-06)
- PostgresHub uses single pgxlisten connection for all LISTEN/NOTIFY
- Non-blocking fan-out with backpressure (drop + refresh signal) (SSE-05)
- Start blocks until context cancellation (SSE-03 graceful shutdown)
- Subscription.Close() unsubscribes and cleans up channel
</success_criteria>

<output>
After completion, create `.planning/phases/08-background-jobs-production-readiness/08-03-SUMMARY.md`
</output>
